{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import re,string\n",
    "import nltk\n",
    "from patsy import dmatrices\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Scraper/pros\"\n",
    "infile = open(filename,'rb')\n",
    "pros_list = pk.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Scraper/cons\"\n",
    "infile = open(filename,'rb')\n",
    "cons_list = pk.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros_clean = [re.sub(r'[^\\w\\s]', ' ', y.lower()) for x in pros_list for y in x]\n",
    "pros_clean = [re.sub(r'[\\s+\\n+]', ' ', x) for x in pros_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_clean = [re.sub(r'[^\\w\\s]', ' ', y.lower()) for x in cons_list for y in x]\n",
    "cons_clean = [re.sub(r'[\\s+\\n+]', ' ', x) for x in cons_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "punc = string.punctuation\n",
    "pros_tokenize = [word for x in pros_clean for word in word_tokenize(x) if word not in stop if word not in punc]\n",
    "cons_tokenize = [word for x in cons_clean for word in word_tokenize(x) if word not in stop if word not in punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\Anaconda2\\envs\\Python3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # need to do 'pip3 install twython' for this to work properly\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "proFreqs = nltk.FreqDist(pros_tokenize)\n",
    "conFreqs = nltk.FreqDist(cons_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('work', 1103),\n",
       " ('great', 705),\n",
       " ('good', 679),\n",
       " ('benefits', 587),\n",
       " ('pay', 507),\n",
       " ('reviews', 505),\n",
       " ('get', 479),\n",
       " ('people', 412),\n",
       " ('working', 406),\n",
       " ('lot', 389)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proFreqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('work', 1302),\n",
       " ('life', 676),\n",
       " ('balance', 643),\n",
       " ('reviews', 504),\n",
       " ('hours', 342),\n",
       " ('get', 325),\n",
       " ('cons', 315),\n",
       " ('time', 308),\n",
       " ('management', 288),\n",
       " ('worked', 287)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conFreqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "confinder2 = BigramCollocationFinder.from_words(cons_tokenize)\n",
    "confinder2.apply_freq_filter(10)\n",
    "contop10_2grams = confinder2.nbest(bigram_measures.pmi, 10)\n",
    "con_scored_2 = confinder2.score_ngrams(bigram_measures.pmi)\n",
    "con_sorted = sorted(con_scored_2, key = lambda x: x[1])\n",
    "contop20_2grams_sorted = con_sorted[-20:]\n",
    "print(contop20_2grams_sorted)\n",
    "with open('cons_wordle.txt', 'w') as f:\n",
    "    for item in contop20_2grams_sorted:\n",
    "        f.write(' '.join(item[0]) + ':' + str(item[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mandatory', 'overtime'),\n",
       " ('physically', 'demanding'),\n",
       " ('peak', 'season'),\n",
       " ('6', 'months'),\n",
       " ('fast', 'paced'),\n",
       " ('leadership', 'principles'),\n",
       " ('tier', '1'),\n",
       " ('night', 'shift'),\n",
       " ('even', 'though'),\n",
       " ('short', 'breaks')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confinder2.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxW = max([gram[1] for gram in contop20_2grams_sorted])\n",
    "minW = min([gram[1] for gram in contop20_2grams_sorted])\n",
    "contop20 = []\n",
    "for gram in contop20_2grams_sorted:\n",
    "    contop20.append([gram[0], (gram[1]-minW+5)/(maxW-minW)])\n",
    "\n",
    "with open('cons 2 grams worlde.txt', 'w') as writeFile:\n",
    "    for gram in contop20:\n",
    "        writeFile.write(' '.join(gram[0]) + ':' + str(gram[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_maker(tokenizedList,filename,num):\n",
    "    bg = BigramCollocationFinder.from_words(tokenizedList)\n",
    "    bg.apply_freq_filter(10)\n",
    "    bg_measures = bg.score_ngrams(bigram_measures.pmi)\n",
    "    bg_sorted = sorted(bg_measures, key = lambda x: x[1])\n",
    "    bg_sorted_20 = bg_sorted[-num:][::-1]\n",
    "    print(\"Top 20 bigrams: \\n\")\n",
    "    print(bg_sorted_20)\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in bg_sorted_20:\n",
    "            f.write(' '.join(item[0]) + ':' + str(item[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 bigrams: \n",
      "\n",
      "[(('dress', 'code'), 10.683955913757938), (('cutting', 'edge'), 10.498089368446605), (('laid', 'back'), 10.135519289061897), (('minimum', 'wage'), 9.890406791225367), (('decision', 'making'), 9.479473690279258), (('co', 'workers'), 9.189967073084278), (('dental', 'vision'), 9.032425796097796), (('10', 'hour'), 8.355484973003804), (('customer', 'obsession'), 8.307758156342455), (('leadership', 'principles'), 8.250996506481835)]\n",
      "Top 20 bigrams: \n",
      "\n",
      "[(('mandatory', 'overtime'), 9.378230365061858), (('physically', 'demanding'), 8.712848162002643), (('peak', 'season'), 8.694392614257897), (('6', 'months'), 8.629297586036012), (('fast', 'paced'), 8.607062367561289), (('leadership', 'principles'), 8.292869921453534), (('tier', '1'), 8.261854831896276), (('night', 'shift'), 8.036721901204979), (('even', 'though'), 7.727272745599308), (('short', 'breaks'), 7.635433162826663)]\n"
     ]
    }
   ],
   "source": [
    "#bigram_maker(cons_tokenize,\"cons_word.txt\")\n",
    "bigram_maker(pros_tokenize,\"pros_bigram_word.txt\",10)\n",
    "bigram_maker(cons_tokenize,\"cond_bigram_word.txt\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_maker(tokenizedList,filename,num):\n",
    "    tg = TrigramCollocationFinder.from_words(tokenizedList)\n",
    "    tg.apply_freq_filter(10)\n",
    "    tg_measures = tg.score_ngrams(trigram_measures.pmi)\n",
    "    tg_sorted = sorted(tg_measures, key = lambda x: x[1])\n",
    "    tg_sorted_20 = tg_sorted[-num:][::-1]\n",
    "    print(\"Top 20 Trigrams: \\n\")\n",
    "    print(tg_sorted_20)\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in tg_sorted_20:\n",
    "            f.write(' '.join(item[0]) + ':' + str(item[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******pros******\n",
      "Top 20 Trigrams: \n",
      "\n",
      "[(('4', 'days', 'week'), 14.113308873651796), (('short', 'duration', '500'), 13.753685109500257), (('life', 'balance', '520'), 13.271409689390381), (('lot', 'short', 'duration'), 13.127338688013978), (('pros', 'working', 'towards'), 13.020705720103699), (('working', 'towards', 'better'), 12.931976801509368), (('duration', '500', 'reviews'), 12.823394081311662), (('balance', '520', 'reviews'), 12.57667233061347), (('learn', 'lot', 'short'), 12.530880548454991), (('fast', 'paced', 'environment'), 12.34763639344867), (('time', 'unpaid', 'time'), 12.177929173732313), (('520', 'reviews', 'get'), 11.896792159075392), (('get', 'learn', 'lot'), 11.67106489957537), (('towards', 'better', 'work'), 11.49009564310316), (('work', 'life', 'balance'), 11.358949468221244), (('reviews', 'get', 'learn'), 11.294551666922406), (('better', 'work', 'life'), 11.193378111013079), (('day', 'work', 'week'), 10.541862010920287), (('benefits', 'day', '1'), 10.520876122175636), (('4', 'day', 'work'), 10.37955019358785)]\n",
      "******cons******\n",
      "Top 20 Trigrams: \n",
      "\n",
      "[(('10', 'hour', 'shifts'), 15.044695901411878), (('seems', 'comparable', 'places'), 14.009594749921511), (('worked', 'worse', '2397'), 13.948379804525285), (('comparable', 'places', 'worked'), 13.947975631657847), (('places', 'worked', 'worse'), 13.914028299734507), (('worse', '2397', 'reviews'), 13.136006807701058), (('issues', '500', 'reviews'), 13.075771700567689), (('balance', 'issues', '500'), 12.724376696739657), (('balance', 'seems', 'comparable'), 12.675273259452425), (('life', 'balance', 'issues'), 11.295040814408246), (('life', 'balance', 'seems'), 11.251673746670157), (('2397', 'reviews', 'work'), 10.80072032995868), (('work', 'life', 'balance'), 10.340767522650506), (('cons', 'work', 'life'), 10.03791473085721), (('reviews', 'work', 'life'), 9.394194330535353), (('500', 'reviews', 'long'), 8.470241236651301), (('show', 'cons', 'work'), 7.689738129756655), (('work', 'long', 'hours'), 6.780660367917697), (('500', 'reviews', 'work'), 6.5411581160316885)]\n"
     ]
    }
   ],
   "source": [
    "print(\"******pros******\")\n",
    "trigram_maker(pros_tokenize,\"pros_trigram_word.txt\",20)\n",
    "print(\"******cons******\")\n",
    "trigram_maker(cons_tokenize,\"cons_trigram_word.txt\",20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting frequent words - bigrams and tri grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lift scores - positive and negative attributes\n",
    "* Topic Modeling\n",
    "* Sentiment\n",
    "* Cosine Similarity with mission statement\n",
    "* Conditional Random Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rating vs. word length\n",
    "* Recommendation by department, location\n",
    "* Titles - Most frequent words in title review\n",
    "* Most frequent words in highly rated reviews\n",
    "* Most frequent words in low rated reviews\n",
    "* Plot sentiments with time, anything stands out\n",
    "* Plot sentiments across job titles, location - US Map of sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment, topics, word length, frequent words across:\n",
    "* High / low ratings - reviews\n",
    "* CEO\n",
    "* Outlook\n",
    "* Recommends\n",
    "* Title - for high/low ratings\n",
    "* Job function\n",
    "* Location\n",
    "* Current / Former employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
